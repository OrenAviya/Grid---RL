
import numpy as np
import random

# נתונים לדוגמה
# w = 4
# h = 3
# L = [(1, 1, 0), (0, 3, 1), (1, 3, -1)]
# p = 0.8
# r = -1
# gamma = 0.5  # גורם הנחה

# דוגמה לשימוש בפונקציה עם הנתונים הנתונים לדוגמה:

def transform_coordinates(L, w, h):
    # Assuming L is a list of (x, y, v) tuples
    return [(h - 1 - y, x, v) for x, y, v in L]  # Adjusted for numpy indexing

w = 7
h = 7
L = [(1,1,-4),(1,5,-6),(5,1,1),(5,5,4)]
p = 0.8
r = -0.5


gamma = 0.5

transformed_L = transform_coordinates(L, w, h)
L = transformed_L  
print ("L: " ,L)

def create_grid(h, w, L):
    grid = np.zeros((h, w))
    for (x, y, reward) in L:
        grid[x, y] = reward
    return grid

grid = create_grid(h, w, L)
print("grid = ", grid)

def value_iteration(grid, p, r, gamma, theta=0.01):
    W, H = grid.shape
    V = np.zeros((W, H))
    policy = np.zeros((W, H), dtype=str)
    actions = ['up', 'down', 'left', 'right']
    delta = float('inf')
    goal_cells = {(gx, gy) for gx, gy, v in L}
    while delta > theta:
        delta = 0
        for x in range(W):
            for y in range(H):
                v = V[x, y]
                max_value = float('-inf')
                best_action = None
                for action in actions:
                    new_x, new_y = x, y
                    if action == 'up':
                        new_x = max(x - 1, 0)
                    elif action == 'down':
                        new_x = min(x + 1, W - 1)
                    elif action == 'left':
                        new_y = max(y - 1, 0)
                    elif action == 'right':
                        new_y = min(y + 1, H - 1)
                    
                    reward = grid[new_x, new_y]
                    value = p * (reward + gamma * V[new_x, new_y]) + (1 - p) * (reward + gamma * V[x, y]) - r
                    if value > max_value:
                        max_value = value
                        best_action = action
                
                V[x, y] = max_value
    
                if (x, y) in goal_cells:
                    policy[x, y] = 'Exit'
                if (x, y) in [(gx, gy) for gx, gy, v in L if v == 0]:
                    policy[x, y] = 'Wall'
                elif (x, y) not in goal_cells :  # Calculate the best action for valid cells
                    policy[x, y] = best_action

                delta = max(delta, abs(v - V[x, y]))
    
    return V, policy

def bellman_equation(grid, p, r, gamma, theta=0.01):
    W, H = grid.shape
    V = np.zeros((W, H))
    delta = float('inf')
    
    while delta > theta:
        delta = 0
        for x in range(W):
            for y in range(H):
                v = V[x, y]
                max_value = float('-inf')
                for action in ['up', 'down', 'left', 'right']:
                    new_x, new_y = x, y
                    if action == 'up':
                        new_x = max(x - 1, 0)
                    elif action == 'down':
                        new_x = min(x + 1, W - 1)
                    elif action == 'left':
                        new_y = max(y - 1, 0)
                    elif action == 'right':
                        new_y = min(y + 1, H - 1)
                    
                    reward = grid[new_x, new_y]
                    value = p * (reward + gamma * V[new_x, new_y]) + (1 - p) * (reward + gamma * V[x, y]) - r
                    max_value = max(max_value, value)
                
                V[x, y] = max_value
                delta = max(delta, abs(v - V[x, y]))
    
    return V

def q_learning(grid, p, r, gamma, alpha=0.01, epsilon=0.1, max_episodes=15000, max_steps=200):
    W, H = grid.shape
    Q = np.zeros((W, H, 4))
    for x, y, v in L:
        if v == 0: 
            Q[x, y, :] = -np.inf

    actions = ['up', 'down', 'left', 'right']
    action_indices = {'up': 0, 'down': 1, 'left': 2, 'right': 3}
    moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}

    def is_valid_move(x, y):
        return 0 <= x < h and 0 <= y < w and (x, y) not in [(gx, gy) for gx, gy, v in L if v == 0]

    def choose_action(state, epsilon):
        if random.uniform(0, 1) < epsilon:
            return random.choice(actions)
        else:
            return actions[np.argmax(Q[state])]
    
    # Initialize policy array before the loop
    policy = np.empty((W, H), dtype=object)

    for episode in range(max_episodes):
        x, y = random.randint(0, h - 1), random.randint(0, w - 1)
        for step in range(max_steps):
            action = choose_action((x, y), epsilon)

            new_x, new_y = x + moves[action][0], y + moves[action][1]

            if not is_valid_move(new_x, new_y):
                new_x, new_y = x, y  

            if random.uniform(0, 1) < p:
                x, y = new_x, new_y
            else:
                alternative_moves = ['left', 'right'] if action in ['up', 'down'] else ['up', 'down']
                alt_move = random.choice(alternative_moves)
                x_alt, y_alt = x + moves[alt_move][0], y + moves[alt_move][1]
                if is_valid_move(x_alt, y_alt):
                    x, y = x_alt, y_alt

            reward = next((v for gx, gy, v in L if gx == x and gy == y), r)

            # Penalize for revisiting a state too often
            if np.sum(np.abs(Q[new_x, new_y] - Q[x, y])) < 0.01:
                reward -= 0.1

            # Update Q-values
            best_next_action = np.argmax(Q[new_x, new_y])
            td_target = reward + gamma * Q[new_x, new_y, best_next_action]
            td_error = td_target - Q[x, y, action_indices[action]]
            Q[x, y, action_indices[action]] += alpha * td_error

            x, y = new_x, new_y
            # Check for terminal state (reward or penalty) and update policy accordingly
            if reward != r:
                if (x, y) in [(gx, gy) for gx, gy, v in L if v == 0]: # Check if it's a wall
                    policy[x, y] = 'Wall'
                else:
                    policy[x, y] = 'Exit'
                break


    V = np.max(Q, axis=2)
    # for x in range(W):
    #     for y in range(H):
    #         if (x, y, grid[x, y]) in L and (x, y) not in [(gx, gy) for gx, gy, v in L if v == 0] :
    #             policy[x, y] = 'Exit'

    return V, policy

def q_values_to_policy(V, L, w, h):
    actions = ['up', 'down', 'left', 'right']
    moves = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}

    def is_valid_move(x, y):
        return 0 <= x < h and 0 <= y < w and (x, y) not in [(gx, gy) for gx, gy, v in L if v == 0]

    policy = np.full((h, w), None, dtype=object)

    goal_cells = {(gx, gy) for gx, gy, v in L}
    for x in range(h):
        for y in range(w):
            if (x, y) in goal_cells:
                policy[x, y] = 'Exit'
            if (x, y) in [(gx, gy) for gx, gy, v in L if v == 0]:
                policy[x, y] = 'Wall'
            elif (x, y) not in goal_cells :  # Calculate the best action for valid cells
                max_value = -np.inf
                best_action = None
                for action in actions:
                    next_x, next_y = x + moves[action][0], y + moves[action][1]
                    if is_valid_move(next_x, next_y) and V[next_x, next_y] > max_value:
                        max_value = V[next_x, next_y]
                        best_action = action
                policy[x, y] = best_action

    return policy

def print_reversed_values(V):
    for row in V[::-1]:
        print(row)

# Run Bellman Iteration
V_bellman = bellman_equation(grid, p, r, gamma)
print("Bellman Value Function:")
print(V_bellman)

# Run Value Iteration (Model-Based RL)
V_value_iter, policy_value_iter = value_iteration(grid, p, r, gamma)
print("Value Iteration Value Function:")
print(V_value_iter)
print("Value Iteration Policy:")
print(policy_value_iter)

# Run Q-Learning (Model-Free RL)
V, policy = q_learning(grid, p, r, gamma)
print("Q-Learning Value Function:")
print(V)
# policy improvement
policy = q_values_to_policy(V, L, w, h)
print("Q-Learning Policy:")
print(policy)